{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSglhl+Z705UHJw3NzA1qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphax/solar-data/blob/jan08-more-clean-data/nb/Clean_GMP_Solar_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "This notebook pre-processes raw downloads from https://greenmountainpower.com/account/usage and produces one cleansed file, suitable for further analysis by the `Solar Viz` notebook.\n",
        "\n",
        "To run this script, you need access to Google Drive, and you need to copy the data from https://github.com/syphax/solar-data/tree/main/data to `/My Drive/Data/Solar` (or edit the path variable in the 2nd code block to point somewhere else).\n",
        "\n",
        "_TODO: Load the data directly from the GitHub repo._\n",
        "\n"
      ],
      "metadata": {
        "id": "hm-1FB99aNTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "52wes2ObaS6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrPc62eAZ1-l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import altair as alt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can of course edit this to taste:\n",
        "\n",
        "path = '/content/drive/MyDrive/Data/Solar/'"
      ],
      "metadata": {
        "id": "J_6wRHUNgeOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will require you to click through a couple windows to \n",
        "# give permission to access your GDrive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KT_RZuhfgi5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n",
        "\n",
        "This script preps data that was downloaded from [Green Mountain Power's website](https://greenmountainpower.com/account/usage/).\n",
        "\n",
        "GMP has an excellent UI for reporting usage, and provides downloadable data in 15 minute increments (either CSV or Green Button XML). *Unfortunately* it only supports manual data downloads in 15 day (max) chunks.  \n",
        "*Fortunately* it only takes a couple minutes to download several months of data. The manual downloads are just easy enough that I haven't bothered to automate the process.\n",
        "\n",
        "Fields in the CSV downloads are:\n",
        "* `ServiceAgreement`: Account info. Format is `Account Holder / Service / Service Acronym / Account Start Date / Account Status`\n",
        "* `IntervalStart`: Timestamp; format is `yyyy-MM-dd-hh:mm:ss`\n",
        "* `IntervalEnd`: Same, 15 minutes later. Redundant but explicit!\n",
        "* `Quantity`: Amount of electricity generated\n",
        "* `UnitOfMeasure` `kWh`. I love that they have an explicit UoM field!\n"
      ],
      "metadata": {
        "id": "dB-7UiEpavW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Data/Solar/'\n",
        "\n",
        "raw_input_files = os.path.join(path, 'UsageData*.csv')\n",
        "joined_input_file = os.path.join(path, 'full_dataset.csv')\n"
      ],
      "metadata": {
        "id": "pG8ov6U5Z-Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should list the data files that you copied from https://github.com/syphax/solar-data/tree/main/data\n",
        "\n",
        "!ls $path "
      ],
      "metadata": {
        "id": "cjSnnjys58v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This concatenates available data files. We will need to remove possible dupes, and check for completeness.\n",
        "\n",
        "!cat $raw_input_files > $joined_input_file"
      ],
      "metadata": {
        "id": "miwNfoab58gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_energy_data_raw = pd.read_csv(joined_input_file)"
      ],
      "metadata": {
        "id": "x5SOcDl86Inz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and Clean"
      ],
      "metadata": {
        "id": "XWGyMyeog_5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What fields are there?\n",
        "\n",
        "df_energy_data_raw.dtypes"
      ],
      "metadata": {
        "id": "uDA_67Nu6IwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a clean field for kWh values\n",
        "\n",
        "df_energy_data_raw['kWh'] = 0\n",
        "\n",
        "df_energy_data_raw['kWh'] = np.where(df_energy_data_raw['UnitOfMeasure'] == 'kWh', df_energy_data_raw['Quantity'], 0)\n",
        "df_energy_data_raw['kWh'] = df_energy_data_raw['kWh'].astype(np.float64)"
      ],
      "metadata": {
        "id": "HNq9gMc76IzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add date field for interval starts\n",
        "\n",
        "fmt = '%Y-%m-%d-%H:%M:%S'\n",
        "df_energy_data_raw['dt_start'] = pd.to_datetime(df_energy_data_raw['IntervalStart'], format=fmt, errors='coerce').dt.date"
      ],
      "metadata": {
        "id": "mQnun62m4PJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick check of values in df:\n",
        "\n",
        "df_energy_data_raw.groupby(['ServiceAgreement', 'UnitOfMeasure'], as_index=False).agg(cnt_records=('ServiceAgreement','count'), \n",
        "                                                                                      unique_dt=('IntervalStart','nunique'), \n",
        "                                                                                      kwh=('kWh','sum'))"
      ],
      "metadata": {
        "id": "iirlZiRyhGfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract service level code"
      ],
      "metadata": {
        "id": "aDBAOmOgpMA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract service level code\n",
        "\n",
        "p = re.compile('.*/.*/(.*)/.*/.*')\n",
        "\n",
        "sl = df_energy_data_raw['ServiceAgreement'].str.extract(p)\n",
        "\n",
        "df_energy_data_raw['Service'] = sl[0].str.strip()\n"
      ],
      "metadata": {
        "id": "bwBb-cfYh6lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correct timestamps\n",
        "\n",
        "The timestamps in these files appear to be in local time. I discovered this when checking for dupes, and finding extra duplicates around e.g. Nov-07-2021 at 1-2am.\n",
        "\n",
        "To ensure that the timestamps are aligned consistently, we need to adjust for DST. Otherwise, we'll see a funky offset when analyzing production by hour of day."
      ],
      "metadata": {
        "id": "p0ORz9Q6pOh6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSBGaoogpv2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNdYAvW4pv5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for dupes\n",
        "\n",
        "df_dupe_check = df_energy_data_raw.groupby(['Service', 'IntervalStart'], as_index=False).agg(cnt_dupes=('IntervalStart','count'))\n",
        "\n",
        "df_dupe_check = df_dupe_check[df_dupe_check['cnt_dupes'] != 1]\n",
        "\n",
        "df_dupe_records = df_energy_data_raw.merge(df_dupe_check, on=['Service', 'IntervalStart'], how='inner').sort_values(['IntervalStart', 'Service'])"
      ],
      "metadata": {
        "id": "u2caRy4xlYIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dupe_records.groupby('dt_start').agg({'cnt_dupes':'sum'})"
      ],
      "metadata": {
        "id": "wJwxKNrV6LfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_dupe_records)"
      ],
      "metadata": {
        "id": "N5Vlwnddl5p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_energy_data_raw"
      ],
      "metadata": {
        "id": "wRk1uoYSnctA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 reasons for dupes- either the downloaded data files overlap, or we have duplicate timestamps during the \"fall back\" Daylight Savings Time. \n",
        "\n",
        "In order to get aligned solar output data, we need to "
      ],
      "metadata": {
        "id": "rjn8cq2Pndxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sh0 = df_energy_data_raw.shape\n",
        "\n",
        "df_solar_data = df_energy_data_raw #[df_energy_data_raw['Service'] == 'NGEN']\n",
        "\n",
        "# # Remove any errant header rows (one may stick around in the data b/c it's not a dupe of the header row)\n",
        "# df_solar_data = df_solar_data[df_solar_data['ServiceAgreement'] != 'ServiceAgreement']\n",
        "\n",
        "# # Now we don't need that column anymore!\n",
        "# df_solar_data = df_solar_data.drop('ServiceAgreement', axis=1)\n",
        "\n",
        "sh0 = df_solar_data.shape\n",
        "\n",
        "df_solar_data = df_solar_data.drop_duplicates()\n",
        "\n",
        "sh1 = df_solar_data.shape\n",
        "\n",
        "cnt_dupes = sh0[0] - sh1[0]\n",
        "\n",
        "print(\"Removed {:,} duplicate entries; {:,} left.\".format(cnt_dupes, sh1[0]))"
      ],
      "metadata": {
        "id": "xb5hmY9uhNIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop ServiceAgreement (which contains some PID):\n",
        "\n",
        "df_solar_data = df_solar_data.drop('ServiceAgreement', axis=1)"
      ],
      "metadata": {
        "id": "hqRglENrh9p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_solar_data"
      ],
      "metadata": {
        "id": "B5q4t6Lhh0Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Clean Dataset"
      ],
      "metadata": {
        "id": "8JtaVgrT7aGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_solar_data.to_csv(os.path.join(path, 'full_dataset.csv'))"
      ],
      "metadata": {
        "id": "tTSL4WAH62eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEaCqU7R7gJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}